---
title: "Playback Detection Model 3"
author: "Anna Kurtin"
date: "2024-09-20"
output: html_document
---

This is the third version of the detection submodel for the playback data. I'm going through other studies and looking at what affected detection of BBCU or YBCU. 
Covariates to include: 

* date, temperature (Based on Johnson and Benson 2022)
* noise (Johnson and Benson 2023)

Change for this model: No quadratic effect of date!!! 

Naive state model:

$$
z_i \sim Bernoulli(\psi_i)
$$ 

$$
logit(\psi_i) = \alpha_0
$$ 
Detection model with six covariates:  

* b1: date

* b3: wind condition

* b4: starting temperature



$$
y_{ij} \sim Bernoulli(p_{ij} * z_i)
$$

$$
logit(p_{ij}) = \beta_0 + \beta_{1}*date + \beta_3*wind + \beta_4*temp
$$

**Handling of NAs:** omission for detections, imputation for habitat covariates

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jagsUI)
library(rjags)
library(tidyverse)
library(jagshelper)
library(ggplot2)
library(beepr)
library(corrgram)
source("./Function_Scripts/Create_HexCodes_CuckooColorBlind.R")
```

```{r Read in Data}
pb_dat <- read.csv("./Analysis/Analysis_Detection_Probability/Data/2023_PBData_FormatforOccModSCALED_8-6-24.csv")

detections <- pb_dat[,2:4]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA

# get the length of non-nas in the detection data
get_na <- function(x){(max(which(!is.na(x))))}
# Apply this to our data
miss <- apply(detections, 1, get_na)
```


```{R Look at Colinearity}
# Extract dates and format longer to look at colinearity 
dates <- pb_dat %>% select(site_id, date_s1, date_s2, date_s3)
dates_long <- dates %>% pivot_longer(cols = c(date_s1,date_s2,date_s3), names_to = "survey_period",values_to = "date")
dates_long <- dates_long %>% mutate(survey_period = case_when(survey_period == "date_s1" ~ "s1",
                                                              survey_period == "date_s2" ~ "s2",
                                                              survey_period == "date_s3" ~ "s3"))
dates_long <- unite(dates_long,site_survey,c(site_id, survey_period),sep="_",remove=TRUE)

# Extract wind and format longer to look at colinearity 
wind <- pb_dat %>% select(site_id, wind_s1, wind_s2, wind_s3)
wind_long <- wind %>% pivot_longer(cols = c(wind_s1, wind_s2, wind_s3), names_to = "survey_period", values_to = "wind")
wind_long <- wind_long %>% mutate(survey_period = case_when(survey_period == "wind_s1" ~ "s1",
                                                            survey_period == "wind_s2" ~ "s2",
                                                            survey_period == "wind_s3" ~ "s3"))
wind_long <- unite(wind_long, site_survey,c(site_id,survey_period), sep = "_", remove = TRUE)

# Extract temp and format longer to look at colinearity 
temp <- pb_dat %>% select(site_id, temp_s1, temp_s2, temp_s3)
temp_long <- temp %>% pivot_longer(cols = c(temp_s1, temp_s2, temp_s3), names_to = "survey_period", values_to = "temp")
temp_long <- temp_long %>% mutate(survey_period = case_when(survey_period == "temp_s1" ~ "s1",
                                                            survey_period == "temp_s2" ~ "s2",
                                                            survey_period == "temp_s3" ~ "s3"))
temp_long <- unite(temp_long, site_survey, c(site_id, survey_period), sep = "_", remove = TRUE)

covs_long <- left_join(dates_long, wind_long, by = "site_survey") %>% left_join(., temp_long, by = "site_survey")

corrgram(covs_long, order=TRUE, lower.panel=panel.cor,
         upper.panel=panel.pts, text.panel=panel.txt,
         main="Correlations in PB Detection Metrics")
# No colinearities to be worried about
# Unused covariates
obs <- pb_dat %>% select(site_id, num_obs_s1, num_obs_s2, num_obs_s3)
#obs_long <- obs %>% pivot_longer(cols = c(num_obs_s1, 
#                                          num_obs_s2,
#                                          num_obs_s3), names_to = "survey period", values_to = "observers")
time <- pb_dat %>% select(site_id, time_s1, time_s2, time_s3)
```

```{R Setup JAGS Data}
# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences (don't need this since we're not considering the state submodel)
  #Z = z_dat,
  # Detection/nondetection data
  det_data = detections,
  # Date
  covb1 = dates[,2:4],
  # wind condition
  covb3 = wind[,2:4],
  # temp
  covb4 = temp[,2:4],
  n_unit = nrow(detections),
  miss = miss
)

# Set initial values 
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = rnorm(1, 0, 1),
    b1Q = rnorm(1, 0 ,1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}
```

```{R PB Model 1 Structure, eval = FALSE}
cat("
  model{
        # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dlogis(0, 1)
    b1Q ~ dlogis(0, 1)
    b3 ~ dlogis(0, 1)
    b4 ~ dlogis(0, 1)
  
    # Likelihood
    # Loop through each sampling unit
    for(i in 1:n_unit){
  
      #Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      #logit(psi[i]) <- a0 
  
      for(j in 1:miss[i]){
  
      # Just detection model from StrofUseLM10
      #mu[i] <- beta0 + beta1 * cov1[i] + beta1Q * (cov1[i]*cov1[i]) + beta2 * cov2[i] + beta3 * cov3[i] + beta4 * cov4[i] # where is this coming from?????
      #prob[i] <- ilogit(mu[i])
      #y[i] ~ dbin(prob[i], s[i])
  
       # Impute missing detection data
        covb1[i,j] ~ dnorm(0, 1)
        covb3[i,j] ~ dnorm(0, 1)
        covb4[i,j] ~ dnorm(0, 1)
  
        det_data[i,j] ~  dbin(mu_p[i,j],miss[i])  # Create probability density drawn for each site
        mu_p[i,j] <- theta[i,j]
        logit(theta[i,j]) <- p[i,j]
        p[i,j] <- b0 + b1*covb1[i,j] + b3*covb3[i,j] + b4*covb4[i,j] 
        # Det NA
        det_na[i,j] <- det_data[i,j]
  
        #Calculate Bayes p-value - plug the parameters into the equation for each sample of the posterior and use this to create new observations
        Presi[i,j] <- (det_na[i,j] - theta[i,j])^2 # Calculate squared residual of observed data
        y_new[i,j] ~ dbern(mu_p[i,j])             # Simulate observed data
        Presi_new[i,j] <- (y_new[i,j] - theta[i,j])^2 # Calculate squared residual error of simulated data
      }
    }
    
    # Derived parameters
    ## Calculate sum of squared residual errors for observed 
    for(i in 1:n_unit){
       sobs[i] <- sum(Presi[i,1:miss[i]])
       ssim[i] <- sum(Presi_new[i,1:miss[i]])
    }
    
    SSEobs <- sum(sobs[])
    SSEsim <- sum(ssim[])
    p_val <- step(SSEsim - SSEobs)  
    
  }
  ", file = "./Analysis/Analysis_Detection_Probability/Model_Structure_Files/JAGS_PBMod3.txt")

```

```{R Run Model, eval = FALSE, results = FALSE}
fit_3 <- jags(data = jags_data, 
                 model.file = "./Analysis/Analysis_Detection_Probability/Model_Structure_Files/JAGS_PBMod3.txt",
                 parameters.to.save = c("b0", "b1","b1Q","b3", "b4", "SSEsim","SSEobs","p_val"),
                 n.iter = 10000, 
                 n.burnin = 5000, 
                 n.chains = 3,
                 n.thin = 2,
                 inits = init_func)
beep(sound = 3)
saveRDS(fit_3,"./Analysis/Analysis_Detection_Probability/Models_Ran_Outputs/JAGS_PBMod3.txt")
```

```{R}
fit_3 <- readRDS("./Analysis/Analysis_Detection_Probability/Models_Ran_Outputs/JAGS_PBMod3.txt")

```

```{R}
summary(fit_3)
tracedens_jags(fit_3, parmfrow = c(3,3))
```


```{R Make Inference on P Value}
plot(fit_3$sims.list$SSEobs,fit_3$sims.list$SSEsim,xlab="SSEobs",ylab="SSEsim")
abline(0,1,lwd = 2, col = "red")
print(paste0("P value is:",round(fit_3$mean$p_val, 2)))
```
Ok so this looks really bad


```{R Create Violin Plots}
chains_2 <- jags_df(fit_3)

chains_beta <- chains_2 %>% select(b1, b1Q, b3, b4)
colnames(chains_beta) <- c("date", 
                           "quadratic date",
                           "wind_strength",
                           "temp")
chains_beta_l <- chains_beta %>% pivot_longer(cols = c("date", 
                                                       "quadratic date", 
                                                       "wind_strength",
                                                       "temp"),names_to = "Parameter", values_to = "Values")
# Create the violin plot
ggplot() +
  geom_violin(data=chains_beta_l, aes(x = Parameter, y = Values,fill = Parameter),trim = FALSE) +
  labs(title = "Posterior Distribution of Detection Covariates") +
  scale_fill_manual(values = c("date" = palette_8[1], "quadratic date" = palette_8[2],  "wind_strength" = palette_8[5], "temp" = palette_8[6])) + 
  geom_hline(yintercept = 0, linetype = "solid", color = "black") +
  theme_minimal()
```

Signal of variance inflation in the posterior for quadratic date

```{R}
plot(fit_3$sims.list$b0,)

```

```{R Look at Detection Prob Estimates}
p <- round(plogis(fit_3$q50$b0),4)
print(paste0("Detection probability for one survey:",p*100))

n <- 3
p_star <- round(1 - (1 - p)^n,4)
print(paste0("Cumulative estimated detection probability:",p_star*100))
# how does this line up with our naive estimates of how many sites had detections on ARU vs PB
```