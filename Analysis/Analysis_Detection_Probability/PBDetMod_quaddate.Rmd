---
title: "Playback Detection Model 2"
author: "Anna Kurtin"
date: "2024-09-19"
output: html_document
---

This is the second version of the detection submodel for the playback data. I'm going through other studies and looking at what affected detection of BBCU or YBCU. 
Covariates to include: 

* date (as a proxy for round of survey/time during the season like they included), temperature (Based on Johnson and Benson 2022)
* noise (Johnson and Benson 2023)

#### Model Structure: 

Naive state model:

$$
z_i \sim Bernoulli(\psi_i)
$$ 

$$
\psi_i = \alpha_0
$$ 
Detection model with six covariates:  

* b1: date

* b2: quadratic effect of date

* b3: wind condition

* b4: starting temperature



$$
y_{ij} \sim Bernoulli(p_{ij} * z_i)
$$
Equation for manuscript
$$
p_{ij} = \beta_0 + \beta_{1}*date + \beta_{2}*date^2 + \beta_3*wind + \beta_4*temperature
$$

Formula for manuscript
$$
logit(p_{ij}) = \beta_0 + \beta_1 * \text{Julian date}_{ij} + \beta_2 * \text{Julian date}^2_{ij} + \beta_3 * \text{wind}_{ij} + \beta_4 * \text{temperature}_{ij}
$$
**Handling of NAs:** omission for detections, imputation for habitat covariates

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jagsUI)
library(rjags)
library(tidyverse)
library(jagshelper)
library(ggplot2)
library(ggdist)
library(corrgram)
source("./Function_Scripts/Create_HexCodes_CuckooColorBlind.R")
```

```{r Read in Data, warnings = FALSE, message = FALSE}
pb_dat <- read.csv("./Analysis/Analysis_Detection_Probability/Data/2023_PBData_FormatforOccModSCALED_8-6-24.csv")

detections <- pb_dat[,2:4]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA

# get the length of non-nas in the detection data
get_na <- function(x){(max(which(!is.na(x))))}
# Apply this to our data
miss <- apply(detections, 1, get_na)
```

*Ignore the following warnings produced by corrgram, I can't get markdown to not show them*

```{R Look at Colinearity, echo = FALSE, warnings = FALSE, message = FALSE}
# Extract dates and format longer to look at colinearity 
dates <- pb_dat %>% select(site_id, date_s1, date_s2, date_s3)
dates_long <- dates %>% pivot_longer(cols = c(date_s1,date_s2,date_s3), names_to = "survey_period",values_to = "date")
dates_long <- dates_long %>% mutate(survey_period = case_when(survey_period == "date_s1" ~ "s1",
                                                              survey_period == "date_s2" ~ "s2",
                                                              survey_period == "date_s3" ~ "s3"))
dates_long <- unite(dates_long,site_survey,c(site_id, survey_period),sep="_",remove=TRUE)

# Extract wind and format longer to look at colinearity 
wind <- pb_dat %>% select(site_id, wind_s1, wind_s2, wind_s3)
wind_long <- wind %>% pivot_longer(cols = c(wind_s1, wind_s2, wind_s3), names_to = "survey_period", values_to = "wind")
wind_long <- wind_long %>% mutate(survey_period = case_when(survey_period == "wind_s1" ~ "s1",
                                                            survey_period == "wind_s2" ~ "s2",
                                                            survey_period == "wind_s3" ~ "s3"))
wind_long <- unite(wind_long, site_survey,c(site_id,survey_period), sep = "_", remove = TRUE)

# Extract temp and format longer to look at colinearity 
temp <- pb_dat %>% select(site_id, temp_s1, temp_s2, temp_s3)
temp_long <- temp %>% pivot_longer(cols = c(temp_s1, temp_s2, temp_s3), names_to = "survey_period", values_to = "temp")
temp_long <- temp_long %>% mutate(survey_period = case_when(survey_period == "temp_s1" ~ "s1",
                                                            survey_period == "temp_s2" ~ "s2",
                                                            survey_period == "temp_s3" ~ "s3"))
temp_long <- unite(temp_long, site_survey, c(site_id, survey_period), sep = "_", remove = TRUE)

covs_long <- left_join(dates_long, wind_long, by = "site_survey") %>% left_join(., temp_long, by = "site_survey")

corrgram(covs_long, order=TRUE, lower.panel=panel.cor,
         upper.panel=panel.pts, text.panel=panel.txt,
         main="Correlations in PB Detection Metrics")

# Unused
obs <- pb_dat %>% select(site_id, num_obs_s1, num_obs_s2, num_obs_s3)
time <- pb_dat %>% select(site_id, time_s1, time_s2, time_s3)
```

No correlations here to worry about (all r < 0.6)


### Set Up and Run Model

```{R Setup JAGS Data}
# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections,
  # Date
  covb1 = dates[,2:4],
  # wind condition
  covb3 = wind[,2:4],
  # temp
  covb4 = temp[,2:4],
  n_unit = nrow(detections),
  miss = miss
)

# Set initial values 
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = rnorm(1, 0, 1),
    b1Q = rnorm(1, 0 ,1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}
```

```{R PB Model 1 Structure, eval = FALSE}
cat("
  model{
        # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dlogis(0, 1)
    b1Q ~ dlogis(0, 1)
    b3 ~ dlogis(0, 1)
    b4 ~ dlogis(0, 1)
  
    # Likelihood
    # Loop through each sampling unit
    for(i in 1:n_unit){
  
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0 
  
      for(j in 1:miss[i]){
       # Impute missing detection data
        covb1[i,j] ~ dnorm(0, 1)
        covb3[i,j] ~ dnorm(0, 1)
        covb4[i,j] ~ dnorm(0, 1)
  
        det_data[i,j] ~  dbern(mu_p[i,j])  # Create probability density drawn for each site
        mu_p[i,j] <- Z[i]*theta[i,j]
        logit(theta[i,j]) <- p[i,j]
        p[i,j] <- b0 + b1*covb1[i,j] + b1Q*(covb1[i,j]^2) + b3*covb3[i,j] + b4*covb4[i,j] 
        # Det NA
        det_na[i,j] <- det_data[i,j]
  
        #Calculate Bayes p-value - plug the parameters into the equation for each sample of the posterior and use this to create new observations
        Presi[i,j] <- (det_na[i,j] - theta[i,j])^2 # Calculate squared residual of observed data
        y_new[i,j] ~ dbern(mu_p[i,j])             # Simulate observed data
        Presi_new[i,j] <- (y_new[i,j] - theta[i,j])^2 # Calculate squared residual error of simulated data
      }
    }
    
    # Derived parameters
    ## Calculate sum of squared residual errors for observed 
    for(i in 1:n_unit){
       sobs[i] <- sum(Presi[i,1:miss[i]])
       ssim[i] <- sum(Presi_new[i,1:miss[i]])
    }
    
    SSEobs <- sum(sobs[])
    SSEsim <- sum(ssim[])
    p_val <- step(SSEsim - SSEobs)  
    
  }
  ", file = "./Analysis/Analysis_Detection_Probability/Model_Structure_Files/JAGS_PBMod2.txt")

```

```{R Run Model, eval = FALSE, results = FALSE}
fit_2 <- jags(data = jags_data, 
                 model.file = "./Analysis/Analysis_Detection_Probability/Model_Structure_Files/JAGS_PBMod2.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b1Q", "b3", "b4","SSEsim","SSEobs", "p_val"),
                 n.iter = 10000, 
                 n.burnin = 5000, 
                 n.chains = 3,
                 n.thin = 2,
                 inits = init_func)
beep(sound = 3)
saveRDS(fit_2,"./Analysis/Analysis_Detection_Probability/Models_Ran_Outputs/JAGS_PBMod2.txt")
```

```{R, echo = FALSE}
fit_2 <- readRDS("./Analysis/Analysis_Detection_Probability/Models_Ran_Outputs/JAGS_PBMod2.txt")
```

```{R, echo = FALSE}
summary(fit_2)
```

### Check Convergence and Model Fit 

```{R, echo = FALSE}
tracedens_jags(fit_2, parmfrow = c(2,2))
```

```{R Make Inference on P Value, echo = FALSE}
plot(fit_2$sims.list$SSEobs,fit_2$sims.list$SSEsim,xlab="SSEobs",ylab="SSEsim")
abline(0,1,lwd = 2, col = "red")
print(paste0("P value is:",round(fit_2$mean$p_val, 2)))
```

Convergence and fit look good.

### Evaluate Posterior Distributions

```{R Create Violin Plots, echo = FALSE}
chains_2 <- jags_df(fit_2)

chains_beta <- chains_2 %>% select(b1, b1Q, b3, b4)

# select intercept estimate
posterior_b0 <- chains_2 %>% select(b0)
#boxplot(posterior_b0, col = cuckoo_palette[1],main = "Posterior Estimates for Detection Probability", horizontal = TRUE)
#boxplot(posterior_b0, col = cuckoo_palette[1],main = "Posterior Estimates for Detection Probability", horizontal = TRUE)
# Create the slab interval plot
ggplot(data = posterior_b0, aes(x = b0)) + 
  # Plot posterior distributions
  stat_slabinterval(fill = cuckoo_palette[1]) +
  # Add a line for 0 to show overlap of posterior
  geom_vline(xintercept = 0) +
  # Remove background color from plots
  theme_minimal() +
  # Turn off the legend
  guides(fill = "none") +
  scale_x_continuous(limits = c(-6,8))

colnames(chains_beta) <- c("date", 
                           "quadratic date",
                           "wind_strength",
                           "temp")
chains_beta_l <- chains_beta %>% pivot_longer(cols = c("date", 
                                                       "quadratic date", 
                                                       "wind_strength",
                                                       "temp"),names_to = "parameter", values_to = "values")

f_stat <- data.frame(
  parameter = c("date", 
                "quadratic date", 
                "wind_strength",
                "temp"),
  median_value = c(paste0("F: ", round(fit_2$f$b1,2)), 
                   paste0("F: ",round(fit_2$f$b1Q,2)), 
                   paste0("F: ",round(fit_2$f$b3,2)), 
                   paste0("F: ",round(fit_2$f$b4,2)))
)
# Create the density plots
# Create the slab interval plot
ggplot(data = chains_beta_l, aes(x = values, y = parameter, fill = parameter)) + 
  # Plot posterior distributions
  stat_slabinterval() +
  # Establish colors
  scale_fill_manual(values = c("temp" = palette_5[2],
                               "wind_strength"=palette_5[3], 
                               "date" = palette_5[4], 
                               "quadratic date" = palette_5[5])) +
  # Add a line for 0 to show overlap of posterior
  geom_vline(xintercept = 0) +
  # Remove background color from plots
  theme_minimal() +
  # Turn off the legend
  guides(fill = "none")+
  # Add median values as text labels
  geom_text(data = f_stat, aes(x = 3.5, y = parameter, label = median_value), color = "black", hjust = -3) 
```

```{R Posterior Estimates}
b1 <- round(plogis(fit_2$q50$b0 + fit_2$q50$b4), 3)
print(paste0("date:", b1))

b1Q <- round(plogis(fit_2$q50$b0 + fit_2$q50$b1Q), 3)
print(paste0("quadratic effect of date:", b1Q))
```

Detection probability estimates:

```{R Look at Detection Prob Estimates, echo = FALSE}
p <- round(plogis(fit_2$q50$b0),4)
print(paste0("Detection probability for one survey: ",p*100))

n <- 3
p_star <- round(1 - (1 - p)^n,4)
print(paste0("Cumulative estimated detection probability: ",p_star*100))
# how does this line up with our naive estimates of how many sites had detections on ARU vs PB
```
